@inproceedings{agostinelli2025ultraformer,
  title={UltraFormer: An Efficient Transformer for FPGAs},
  author={Agostinelli, Victor and Agostini, Nicolas Bohm and Tumeo, Antonino},
  booktitle={2025 IEEE 33rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={274--274},
  year={2025},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/11008930?casa_token=teC4JpwzrQsAAAAA:c1pGAI_jwjGOSMEC1bcJoJgK37bjrhtcejafENfBSxmKXXk0SoS6uZ22Xk_tE37ErdD88fHqWRQ},
  abbr={FCCM '25}
}

@misc{baartmans2025mlhardwaredesigninterpretability,
      title={ML For Hardware Design Interpretability: Challenges and Opportunities}, 
      author={Raymond Baartmans and Andrew Ensinger and Victor Agostinelli and Lizhong Chen},
      year={2025},
      eprint={2504.08852},
      arxiv={2504.08852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      abstract={The increasing size and complexity of machine learning (ML) models have driven the growing need for custom hardware accelerators capable of efficiently supporting ML workloads. However, the design of such accelerators remains a time-consuming process, heavily relying on engineers to manually ensure design interpretability through clear documentation and effective communication. Recent advances in large language models (LLMs) offer a promising opportunity to automate these design interpretability tasks, particularly the generation of natural language descriptions for register-transfer level (RTL) code, what we refer to as "RTL-to-NL tasks." In this paper, we examine how design interpretability, particularly in RTL-to-NL tasks, influences the efficiency of the hardware design process. We review existing work adapting LLMs for these tasks, highlight key challenges that remain unaddressed, including those related to data, computation, and model development, and identify opportunities to address them. By doing so, we aim to guide future research in leveraging ML to automate RTL-to-NL tasks and improve hardware design interpretability, thereby accelerating the hardware design process and meeting the increasing demand for custom hardware accelerators in machine learning and beyond.},
      url={https://arxiv.org/abs/2504.08852},
      abbr={arXiv},
}

@misc{ensinger2024swifthighperformancesparsetensor,
      title={Swift: High-Performance Sparse Tensor Contraction for Scientific Applications}, 
      author={Andrew Ensinger and Gabriel Kulp and Victor Agostinelli and Dennis Lyakhov and Lizhong Chen},
      year={2024},
      eprint={2410.10094},
      arxiv={2410.10094},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      abstract={In scientific fields such as quantum computing, physics, chemistry, and machine learning, high dimensional data are typically represented using sparse tensors. Tensor contraction is a popular operation on tensors to exploit meaning or alter the input tensors. Tensor contraction is, however, computationally expensive and grows quadratically with the number of elements. For this reason, specialized algorithms have been created to only operate on the nonzero elements. Current sparse tensor contraction algorithms utilize sub-optimal data structures that perform unnecessary computations which increase execution time and the overall time complexity. We propose Swift, a novel algorithm for sparse tensor contraction that replaces the costly sorting with more efficient grouping, utilizes better data structures to represent tensors, and employs more memory-friendly hash table implementation. Swift is evaluated against the state-of-the-art sparse tensor contraction algorithm, demonstrating up to 20x speedup in various test cases and being able to handle imbalanced input tensors significantly better.},
      url={https://arxiv.org/abs/2410.10094}, 
      abbr={arXiv},
}

@misc{raffel2024simultaneousmaskingpromptingoptimization,
      title={Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation}, 
      author={Matthew Raffel and Victor Agostinelli and Lizhong Chen},
      year={2024},
      eprint={2405.10443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.10443},
      abstract={Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.},
      code={https://github.com/OSU-STARLAB/Simul-LLM/tree/main/examples/simulmask},
      arxiv={2405.10443},
      selected={true},
      abbr={EMNLP '24},
}

@inproceedings{agostinelli-etal-2024-simul,
      title = "Simul-{LLM}: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models",
      author = "Agostinelli, Victor  and
      Wild, Max  and
      Raffel, Matthew  and
      Fuad, Kazi  and
      Chen, Lizhong",
      editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      pages = "10530--10541",
      abstract = "Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state-of-the-art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model. In this paper, we address key challenges facing LLMs fine-tuned for SimulMT, validate classical SimulMT concepts and practices in the context of LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT, and introduce Simul-LLM, the first open-source fine-tuning and evaluation pipeline development framework for LLMs focused on SimulMT.",
      code={https://github.com/OSU-STARLAB/Simul-LLM},
      arxiv={2312.04691},
      selected={true},
      abbr={ACL '24},
}

@inproceedings{
      agostinelli2024leapformer,
      title={LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions},
      author={Victor Agostinelli and Sanghyun Hong and Lizhong Chen},
      booktitle={Forty-first International Conference on Machine Learning},
      year={2024},
      url={https://openreview.net/forum?id=XhH1OKLANY},
      abstract={A promising approach to preserving model performance in linearized transformers is to employ position-based re-weighting functions. However, state-of-the-art re-weighting functions rely heavily on target sequence lengths, making it difficult or impossible to apply them to autoregressive and simultaneous tasks, where the target and sometimes even the input sequence length are unknown. To address this issue, we propose Learned Proportions (LeaP) and LeaPformers2. Our contribution is built on two major components. First, we generalize the dependence on explicit positional representations and sequence lengths into dependence on sequence proportions for re-weighting. Second, we replace static positional representations with dynamic proportions derived via a compact module, enabling more flexible attention concentration patterns. We evaluate LeaPformer against eight representative efficient transformers on the Long-Range Arena benchmark, showing that LeaPformer achieves the best quality-throughput trade-off, as well as LeaPformer to Wikitext-103 autoregressive language modeling and simultaneous speech-to-text translation for two language pairs, achieving competitive results.},
      code={https://github.com/OSU-STARLAB/LeaPformer},      
      arxiv={2405.13046},
      selected={true},
      abbr={ICML '24},
}

@InProceedings{10.1007/978-3-031-43421-1_6,
author="Agostinelli, Victor
and Chen, Lizhong",
editor="Koutra, Danai
and Plant, Claudia
and Gomez Rodriguez, Manuel
and Baralis, Elena
and Bonchi, Francesco",
title="Improving Autoregressive NLP Tasks viaÂ Modular Linearized Attention",
booktitle="Machine Learning and Knowledge Discovery in Databases: Research Track",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="90--106",
abstract="Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or other resource-constrained environment. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer [32], to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.",
isbn="978-3-031-43421-1",
code={https://github.com/OSU-STARLAB/Modular-Linearized-Attention},
arxiv={2304.08453},
abbr={ECML '23},
}

@misc{huang2023partitioningguidedkmeansextremecluster,
      title={Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression}, 
      author={Tianhong Huang and Victor Agostinelli and Lizhong Chen},
      year={2023},
      eprint={2306.14031},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.14031}, 
      abstract={Compactness in deep learning can be critical to a model's viability in low-resource applications, and a common approach to extreme model compression is quantization. We consider Iterative Product Quantization (iPQ) with Quant-Noise to be state-of-the-art in this area, but this quantization framework suffers from preventable inference quality degradation due to prevalent empty clusters. In this paper, we propose several novel enhancements aiming to improve the accuracy of iPQ with Quant-Noise by focusing on resolving empty clusters. Our contribution, which we call Partitioning-Guided k-means (PG k-means), is a heavily augmented k-means implementation composed of three main components. First, we propose a partitioning-based pre-assignment strategy that ensures no initial empty clusters and encourages an even weight-to-cluster distribution. Second, we propose an empirically superior empty cluster resolution heuristic executed via cautious partitioning of large clusters. Finally, we construct an optional optimization step that consolidates intuitively dense clusters of weights to ensure shared representation. The proposed approach consistently reduces the number of empty clusters in iPQ with Quant-Noise by 100x on average, uses 8x fewer iterations during empty cluster resolution, and improves overall model accuracy by up to 12%, when applied to RoBERTa on a variety of tasks in the GLUE benchmark.},
      arxiv={2306.14031},
      abbr={arXiv},
}

