@misc{raffel2024simultaneousmaskingpromptingoptimization,
      title={Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation}, 
      author={Matthew Raffel and Victor Agostinelli and Lizhong Chen},
      year={2024},
      eprint={2405.10443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.10443}, 
}

@misc{agostinelli2024simulllmframeworkexploringhighquality,
      title={Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models}, 
      author={Victor Agostinelli and Max Wild and Matthew Raffel and Kazi Ahmed Asif Fuad and Lizhong Chen},
      year={2024},
      eprint={2312.04691},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.04691}, 
}

@inproceedings{
      agostinelli2024leapformer,
      title={LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions},
      author={Victor Agostinelli and Sanghyun Hong and Lizhong Chen},
      booktitle={Forty-first International Conference on Machine Learning},
      year={2024},
      url={https://openreview.net/forum?id=XhH1OKLANY}
}

@InProceedings{10.1007/978-3-031-43421-1_6,
author="Agostinelli, Victor
and Chen, Lizhong",
editor="Koutra, Danai
and Plant, Claudia
and Gomez Rodriguez, Manuel
and Baralis, Elena
and Bonchi, Francesco",
title="Improving Autoregressive NLP Tasks viaÂ Modular Linearized Attention",
booktitle="Machine Learning and Knowledge Discovery in Databases: Research Track",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="90--106",
abstract="Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or other resource-constrained environment. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer [32], to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.",
isbn="978-3-031-43421-1"
}

@misc{huang2023partitioningguidedkmeansextremecluster,
      title={Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression}, 
      author={Tianhong Huang and Victor Agostinelli and Lizhong Chen},
      year={2023},
      eprint={2306.14031},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.14031}, 
}

